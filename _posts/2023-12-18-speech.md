---
# date: 2010-09-10 12:26:40
layout: post
title: Speech Demo
subtitle: This video demo combines visuals created in Unreal engine with a 7OA mix of a performance of an original script about the evolution of sound and spatial audio technology.
description: This video demo combines visuals created in Unreal engine with a 7OA mix of a performance of an original script about the evolution of sound and spatial audio technology.

# image: https://res.cloudinary.com/dhwvtuay6/image/upload/c_scale,w_800/v1706023027/SD_4kSnapshot_nfqe7k.png
optimized_image: https://res.cloudinary.com/dhwvtuay6/image/upload/c_scale,w_360/v1706023027/SD_4kSnapshot_nfqe7k.png
category: demo
tags:
  - ambisonics
  - motion capture
author: Jacob Cooper
---

<iframe
  id="ytplayer"
  type="text/html"
  width="640"
  height="360"
  src="https://www.youtube.com/embed/10jbOD-Xevc?autoplay=0"
  frameborder="0"
  allowfullscreen
></iframe>

## Audio Workflow

### Recording

Recorded using an AKG C414, shock mount and pop filter in an anechoic chamber.

<figure>  
  <img src="https://res.cloudinary.com/dhwvtuay6/image/upload/c_scale,w_700/v1705507835/IMG_5206_keht27.jpg">
  <figcaption>Recording setup used for speech demo.</figcaption>
</figure>

### Sound Effects

Sound effects sourced from SoundSnap.

### Mixing

Mixed using a 7OA workflow in Reaper. Key processing applied is as follows:

#### Pre-processing: iZotope RX9

Spectral de-noise used to remove noise.

Mouth de-click used to remove mouth noises

#### Encoding: 7th Order Encoding

Plug-ins used: IEM MultiEncoder and AmbiX o7 Encoder

#### Convolution: X-MCFX Convolver

Facilitates convolution with real-world Spatial Room Impulse Responses (SRIRs)

#### Reverbs: IEM FdnReverb

Three reverb sends used with varying reverb times: 0.3 s, 2.1 s and 4.5 s

### Deliverable Formats

The audio was provided in the following formats:

- Binaural
- 7.1.4
- 7th Order Ambisonics
- EBU ADM

The BBCâ€™s EAR Production Suite was used to create an EBU ADM file. The EBU ADM file contained 2 HOA files: one for the reverb and the other for the spatial sound effects used; the rest of the sounds were (mono) objects. The automation data was copied from the Ambisonic encoders to the EAR object plug-ins. The stems were also provided separately.

The audio was converted to binaural using decoding filters developed by Tomasz Rudzki. The audio was converted to 7.1.4 using the SPARTA decoder (no audio was rendered to the LFE channel); this used the Dolby Atmos routing (SSL and SSR were sent to outputs 5 & 6).

## Visual Workflow

The visuals were constructed using the following:

- Face-tracking captured using LiveLink Face For Unreal Engine
- Project assembly using Unreal Engine 5

### Face-Tracking

Face-tracking was recorded using the LiveLink Face for Unreal application for Apple devices (with True Depth camera).

### Project Assembly

The tracking data was imported into Unreal and used to animate a metahuman. The locations of the metahuman on screen provide an indicator of the position of the voice.

## Credits

### Recording, Production & Mixing

Jacob Cooper

### Motion-Capture Performance

Stephanie Ornithari Roberts

### Visuals

Joe Rees-Jones